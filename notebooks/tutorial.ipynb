{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bed8aed8",
   "metadata": {},
   "source": [
    "# OHBM 2025 IBMA with NeuroVault tutorial\n",
    "\n",
    "## IBMA: Image-based meta-analysis\n",
    "\n",
    "IBMA, considered the gold standard of neuroimaging meta-analysis (Salimi-Khorshidi et al., 2009; Salo et al., 2023), consists of aggregating results from group-level, whole-brain statistical maps from individually conducted functional magnetic resonance imaging (fMRI) studies. IBMA outperforms other popular meta-analysis methods, such as coordinate-based meta-analysis (CBMA). IBMA methods use whole-brain statistics; thus, all existing voxel-wise statistical methods are available to analyze subject-level data within studies (Lazar et al., 2002). IBMA is known to produce richer and more detailed results, with additional brain structures that are often absent from CBMA results. IBMA also has greater power; thus, one could potentially achieve similar or even better results with a small fraction of studies generally required in CBMA. In addition, when both the parameters and variance estimates are available, hierarchical mixed effect models can be used to account for both within- and between-study variance (Salimi-Khorshidi et al., 2009)\n",
    "\n",
    "## Tools\n",
    "\n",
    "### NiMARE\n",
    "\n",
    "![NiMARE banner](images/nimare_banner.png)\n",
    "\n",
    "[NiMARE](https://nimare.readthedocs.io/en/latest/) is a Python library for performing neuroimaging meta-analyses and related analyses, like automated annotation and functional decoding. The goal of NiMARE is to centralize and standardize implementations of common meta-analytic tools, so that researchers can use whatever tool is most appropriate for a given research question.\n",
    "\n",
    "### NeuroVault\n",
    "\n",
    "![NeuroVault logo](images/neurovault-logo.svg)\n",
    "\n",
    "[NeuroVault](https://neurovault.org) is a web-based repository of fMRI statistical maps from neuroimaging studies (Gorgolewski et al., 2015). The brain maps are grouped in collections that are created and updated voluntarily. This repository can be explored and downloaded with the help of an API, which is supported by some Python neuroimaging tools (e.g., Nilearn and NiMARE).\n",
    "\n",
    "### Cognitive Atlas\n",
    "\n",
    "![CogAt logo](images/cogat-logo.png)\n",
    "\n",
    "[Cognitive Atlas](https://www.cognitiveatlas.org/) (Poldrack et al., 2011) is an online repository of cumulative knowledge from experienced researchers from the psychology, cognitive science, and neuroscience fields. The repository currently offers two knowledge bases: 907 cognitive concepts and 841 tasks with definitions and properties. Cognitive concepts contain relationships with other concepts and tasks, with the goal of establishing a map between mental processes and brain function. It provides an API to download the database, which is also integrated into NiMARE.\n",
    "\n",
    "## Goals for this tutorial\n",
    "\n",
    "1. Familiarize and download with NeuroVault data\n",
    "2. Identify usable Neurovault images for IBMA\n",
    "3. LEarn identify outliers and \n",
    "4. Use NiMARE to run IBMA\n",
    "5. Interpret IBMA results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46d986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os.path as op\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from nimare.reports.base import run_reports\n",
    "from nimare.workflows import IBMAWorkflow\n",
    "import requests\n",
    "\n",
    "from utils import download_images, convert_to_nimare_dataset\n",
    "from outliers import _rm_nonstat_maps, _rm_extreme_maps, _rm_duplicates_maps\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "# Slide on motivations with repect to Neurvault data. potential use cases for this nmotebook.\n",
    "# Soince peole are going to be using it for their own analyss in the fututerr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e5f345",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./data\"\n",
    "filename = \"november_2022\"\n",
    "nv_data_dir = op.join(data_dir, filename)\n",
    "image_dir = op.join(data_dir, \"nv_images\")\n",
    "report_dir = op.join(data_dir, \"report\")\n",
    "report_clean_dir = op.join(data_dir, \"report_clean\")\n",
    "report_clean_clean_dir = op.join(data_dir, \"report_clean_clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66ad6c1",
   "metadata": {},
   "source": [
    "# Download Neurovault data\n",
    "\n",
    "To explore the NeuroVault database, we created an SQL query and exported the database contents to human-readable tables while filtering sensitive user information. This provided sufficient metadata from all collections and images to investigate the entire database without downloading the files. The images identified as usable for IBMA (see the following section on the image selection framework) were downloaded along with their metadata and converted to a NiMARE Dataset object to leverage existing IBMA methods implemented in NiMARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48caca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "github_url = (\n",
    "    f\"https://raw.githubusercontent.com/NeuroVault/nv-data/master/{filename}.tar.gz\"\n",
    ")\n",
    "response = requests.get(github_url)\n",
    "\n",
    "file_path = op.join(data_dir, f\"{filename}.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135306ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, \"wb\") as f:\n",
    "    f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a301fa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tarfile.open(file_path, \"r:gz\") as tar:\n",
    "    tar.extractall(path=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21570bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(os.listdir(nv_data_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5f0131",
   "metadata": {},
   "source": [
    "The files availabels are the following:\n",
    "\n",
    "- `django_content_type.csv`: \n",
    "- `statmaps_atlas.csv`: Contain some atlases available in NeuroVaults\n",
    "- `statmaps_basecollectionitem.csv`: Contain the image IDs, name, description and its associated collections ID\n",
    "- `statmaps_cognitiveatlascontrast.csv`: Contains the contrast names and Ids from Cognitive Atlas\n",
    "- `statmaps_cognitiveatlastask.csv`: Contains the task names and Ids from Cognitive Atlas\n",
    "- `statmaps_collection.csv`: Contain the collection level metadata, including name, DOI, authors, etc.\n",
    "- `statmaps_collection_communities.csv`: Contains the comunities link to a collection\n",
    "- `statmaps_collection_contributors.csv`: Contains the user ID link to a colleciton\n",
    "- `statmaps_community.csv`: Contain the information about communities in NeuroVault, such as their label and description\n",
    "- `statmaps_image.csv`: Contain image file name and demographic information linked to the image\n",
    "- `statmaps_statisticmap.csv`: COntain image metadata such as map type, modality, task, analysis leve, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4efe198",
   "metadata": {},
   "source": [
    "To identify images usable for IBMA were first get the images ID (`id`) and name (`name`), and their affiliated collection ID (`collection_id`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8b242e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get images and collection IDs\n",
    "cmeta_cols = [\"id\", \"collection_id\", \"name\"]\n",
    "image_colection = pd.read_csv(\n",
    "    op.join(nv_data_dir, \"statmaps_basecollectionitem.csv\"),\n",
    "    usecols=cmeta_cols,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c19ab8",
   "metadata": {},
   "source": [
    "Second, from `statmaps_image.csv` we get the file name, which will use later to download the images using the NuroVault API. In this database `basecollectionitem_ptr_id` refers to the image ID `id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc35780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get path to images in NeuroVault\n",
    "image_cols = [\"file\", \"basecollectionitem_ptr_id\"]\n",
    "image = pd.read_csv(\n",
    "    op.join(nv_data_dir, \"statmaps_image.csv\"),\n",
    "    usecols=image_cols,\n",
    ")\n",
    "image = image.rename(columns={\"basecollectionitem_ptr_id\": \"id\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc6913c",
   "metadata": {},
   "source": [
    "Third, extract some relevant images metadata from `statmaps_statisticmap.csv`. The selected metadata included:\n",
    "- `image_ptr_id`: \n",
    "- `map_type`:\n",
    "- `modality`:\n",
    "- `analysis_level`:\n",
    "- `number_of_subjects`:\n",
    "- `is_thresholded`:\n",
    "- `brain_coverage`:\n",
    "- `not_mni`:\n",
    "- `cognitive_paradigm_cogatlas_id`:\n",
    "\n",
    "One can find the comlete list of Cognitive Atlas task and thier IDs in the file `statmaps_cognitiveatlastask.csv`, in the Cognitive Atlas website https://www.cognitiveatlas.org/tasks, which are also acccesible via API in https://www.cognitiveatlas.org/api/v-alpha/task. \n",
    "\n",
    "Here the image ID is identify by `image_ptr_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57d35fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get image metadata\n",
    "imeta_cols = [\n",
    "    \"image_ptr_id\",\n",
    "    \"map_type\",\n",
    "    \"modality\",\n",
    "    \"analysis_level\",\n",
    "    \"number_of_subjects\",\n",
    "    \"is_thresholded\",\n",
    "    \"brain_coverage\",\n",
    "    \"not_mni\",\n",
    "    \"cognitive_paradigm_cogatlas_id\",\n",
    "]\n",
    "image_meta = pd.read_csv(\n",
    "    op.join(nv_data_dir, \"statmaps_statisticmap.csv\"),\n",
    "    usecols=imeta_cols,\n",
    ")\n",
    "image_meta = image_meta.rename(columns={\"image_ptr_id\": \"id\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a4f773",
   "metadata": {},
   "source": [
    "Finally, we combine the previous extracted information into a single Pnadas DataFrame by merging on the image ID (`id`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c524b66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_merge = pd.merge(image_colection, image, how=\"left\", on=\"id\")\n",
    "image_full_df = pd.merge(image_merge, image_meta, how=\"left\", on=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fb5492",
   "metadata": {},
   "source": [
    "# Select images for IBMA\n",
    "\n",
    "Using the available metadata from the retrieved tables, we set different inclusion criteria for images to be considered for a meta-analysis. We focused on fMRI-BOLD images, as they are the most prevalent modality in NeuroVault. Note that the methods presented in this paper should work with other image modalities (e.g., PET, diffusion MRI, structural MRI). Still, only fMRI-BOLD had enough data in NeuroVault for meta-analyses. Then, we specifically chose images from group-level analyses. Additionally, we retained only images from studies with a sample size greater than ten subjects. Next, we selected images classified as T or Z statistics. Although best practices in meta-analysis suggest using meaningful units and incorporating uncertainty through standard errors, T/Z statistic maps are the most commonly shared images in NeuroVault (Maumet and Nichols, 2016). We discuss this further in the following sections. Upon review, it is important to note here that plenty of images in NeuroVault are labeled as “Other” for the image type. Nonetheless, most of those images actually correspond to known image types (e.g., T/Z statistic). As a result, we relabeled those images to their original type if keywords such as “zstat,” “tstat,” “Z_,” or “T_” were present in the image name, file name, or image description. Following that, we retained unthresholded images that cover 40% of the brain and are in MNI space. Ultimately, we narrowed our selection to images associated with a Cognitive Atlas task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b312dcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify columns definitions\n",
    "image_selected_df = image_full_df.query(\n",
    "    'modality == \"fMRI-BOLD\"'\n",
    "    ' & analysis_level == \"G\"'\n",
    "    ' & number_of_subjects > 10'\n",
    "    ' & (map_type == \"Z\" | map_type == \"T\")'\n",
    "    ' & is_thresholded == \"f\"'\n",
    "    ' & brain_coverage > 40'\n",
    "    ' & not_mni == \"f\"'\n",
    "    ' & cognitive_paradigm_cogatlas_id == \"tsk_4a57abb949bcd\"'  # n-back task\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3727f4a8",
   "metadata": {},
   "source": [
    "# Download images for IBMA\n",
    "\n",
    "Next, we attempt to downlod the selected image from NeuroVault, to facilitate that process we implemented a function named download images, which take the images IDs from the previously created Python DataFrame, and a path to a directory to download the images. Note that not all images can be downloaded, some ogf them belong to private collection, while others have a corrupted file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0d9d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep downloaded images only\n",
    "# Explain file name meaning: col_id, and img_id + name of the files as represented in NeuroVault\n",
    "image_ids = image_selected_df[\"id\"].unique()\n",
    "image_usable_df = download_images(image_ids, image_dir)\n",
    "print(f\"Usable images: {len(image_usable_df)}/{len(image_ids)}\")\n",
    "# Focus on input and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd9b2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_df = pd.merge(image_selected_df, image_usable_df, on=\"id\")\n",
    "image_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9697c536",
   "metadata": {},
   "source": [
    "# Create NiMARE Dataset object for IBMA\n",
    "\n",
    "THen, we create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b586255",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = convert_to_nimare_dataset(image_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0c1774",
   "metadata": {},
   "source": [
    "# Initialize an IBMA workflow with NiMARE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102900e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using all the default for teaching popurse\n",
    "# List the default so that people have an understandin of the flexibility\n",
    "workflow = IBMAWorkflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53af63d6",
   "metadata": {},
   "source": [
    "# Apply workflow to a the dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8edea1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = workflow.fit(dset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da05a274",
   "metadata": {},
   "source": [
    "# Generate HTML report of IBMA results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5bb5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_reports(results, report_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461b3eb5",
   "metadata": {},
   "source": [
    "# Eclude images \n",
    "\n",
    "Even after applying the previous strict preliminary inclusion criteria, we still found plenty of wrongly annotated images, especially representing other image modalities and others with extreme values. Therefore, we developed an automatic heuristic selection to remove those spurious images from the meta-analysis. The heuristic selection consisted of two steps. First, we removed all images from collections that lacked a link to a publication. Also, images with a minimum Z value smaller than 1.96 (i.e., Z score for a 0.05 p-value) were removed as they potentially consisted of mislabeled correlation maps, inverted p-value maps, or did not contain voxels statistically significant. We also excluded images with a maximum Z score larger than 50. Although the number 50 is arbitrary, we wanted to detect images with an unusually large signal. For example, mislabeled BOLD or COPE (contrast of parameter estimates) images or others resulting from studies with a huge sample size. Additionally, using the image metadata, we analyzed the image and file name. We removed those containing keywords such as \"ICA,\" \"PCA,\" \"PPI,\" \"seed,\" \"functional connectivity,\" “cope,” “tfce,” and \"correlation,\" which represent modalities not of interest for the meta-analysis of the current work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be66dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_clean_df = _rm_nonstat_maps(image_df)\n",
    "print(image_clean_df.shape)\n",
    "image_clean_df = _rm_extreme_maps(image_clean_df)\n",
    "image_clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fd3ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_clean_clean = _rm_duplicates_maps(dset_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1e55ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_clean_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c296b249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making explicit we are trasforming Z->T and T->Z\n",
    "# Explain why we are using T/Z\n",
    "\n",
    "dset_clean = convert_to_nimare_dataset(image_clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4359479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform heuristic selection of images\n",
    "# Run the meta-analysis and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c830f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0193a30c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118bd149",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecadd463",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_clean = IBMAWorkflow()\n",
    "results_clean = workflow_clean.fit(dset_clean)\n",
    "run_reports(results_clean, report_clean_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591f16e8",
   "metadata": {},
   "source": [
    "It is quite common for NeuroVault users to upload inverted contrasts and duplicates. For example, one might find two images representing the same contrast (such as House > Face) but with the signs reversed (i.e., Face > House). This creates problems for meta-analyses, as these images effectively cancel each other out when aggregated. Additionally, it is typical for users to upload multiple images of the same contrast, differing only by the covariate used in the group-level analysis. These can be considered duplicates, especially when the covariate does not influence the final estimate. To identify duplicates, we utilize the correlation matrix of the input samples. Image pairs with a correlation close to 1 are considered duplicates, while those with a correlation close to -1 are labeled as inverted contrasts. From the identified duplicates, we randomly selected one image from each pair. For pairs of inverted contrasts, we choose the image with a positive slope relative to the median image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eedfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nimare.meta.ibma import Stouffers\n",
    "\n",
    "workflow_clean_clean = IBMAWorkflow(\n",
    "    estimator=Stouffers(aggressive_mask=False),\n",
    "    corrector=\"fdr\",\n",
    "    diagnostics=\"jackknife\",\n",
    "    voxel_thresh=3.2,\n",
    "    cluster_threshold=90,\n",
    "    output_dir=None,\n",
    "    n_cores=1,\n",
    ")\n",
    "results_clean_clean = workflow_clean_clean.fit(dset_clean_clean)\n",
    "run_reports(results_clean_clean, report_clean_clean_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c141486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7211d62b",
   "metadata": {},
   "source": [
    "# Next steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87fdf66",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "For additional detail \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
