{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bed8aed8",
   "metadata": {},
   "source": [
    "# OHBM 2025 IBMA with NeuroVault tutorial\n",
    "\n",
    "## IBMA: Image-based meta-analysis\n",
    "\n",
    "IBMA, considered the gold standard of neuroimaging meta-analysis (Salimi-Khorshidi et al., 2009; Salo et al., 2023), consists of aggregating results from group-level, whole-brain statistical maps from individually conducted functional magnetic resonance imaging (fMRI) studies. IBMA outperforms other popular meta-analysis methods, such as coordinate-based meta-analysis (CBMA). IBMA methods use whole-brain statistics; thus, all existing voxel-wise statistical methods are available to analyze subject-level data within studies (Lazar et al., 2002). IBMA is known to produce richer and more detailed results, with additional brain structures that are often absent from CBMA results. IBMA also has greater power; thus, one could potentially achieve similar or even better results with a small fraction of studies generally required in CBMA. In addition, when both the parameters and variance estimates are available, hierarchical mixed effect models can be used to account for both within- and between-study variance (Salimi-Khorshidi et al., 2009)\n",
    "\n",
    "## Tools\n",
    "\n",
    "### NiMARE\n",
    "\n",
    "![NiMARE banner](images/nimare_banner.png)\n",
    "\n",
    "[NiMARE](https://nimare.readthedocs.io/en/latest/) is a Python library for performing neuroimaging meta-analyses and related analyses, like automated annotation and functional decoding. The goal of NiMARE is to centralize and standardize implementations of common meta-analytic tools, so that researchers can use whatever tool is most appropriate for a given research question.\n",
    "\n",
    "### NeuroVault\n",
    "\n",
    "![NeuroVault logo](images/neurovault-logo.svg)\n",
    "\n",
    "[NeuroVault](https://neurovault.org) is a web-based repository of fMRI statistical maps from neuroimaging studies (Gorgolewski et al., 2015). The brain maps are grouped in collections that are created and updated voluntarily. This repository can be explored and downloaded with the help of an API, which is supported by some Python neuroimaging tools (e.g., Nilearn and NiMARE).\n",
    "\n",
    "### Cognitive Atlas\n",
    "\n",
    "![CogAt logo](images/cogat-logo.png)\n",
    "\n",
    "[Cognitive Atlas](https://www.cognitiveatlas.org/) (Poldrack et al., 2011) is an online repository of cumulative knowledge from experienced researchers from the psychology, cognitive science, and neuroscience fields. The repository currently offers two knowledge bases: 907 cognitive concepts and 841 tasks with definitions and properties. Cognitive concepts contain relationships with other concepts and tasks, with the goal of establishing a map between mental processes and brain function. It provides an API to download the database, which is also integrated into NiMARE.\n",
    "\n",
    "## Goals for this tutorial\n",
    "\n",
    "1. Familiarize and download with NeuroVault data\n",
    "2. Identify usable Neurovault images for IBMA\n",
    "3. LEarn identify outliers and \n",
    "4. Use NiMARE to run IBMA\n",
    "5. Interpret IBMA results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46d986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os.path as op\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from nimare.reports.base import run_reports\n",
    "from nimare.workflows import IBMAWorkflow\n",
    "import requests\n",
    "from nimare.meta.ibma import Stouffers\n",
    "\n",
    "from utils import download_images, convert_to_nimare_dataset\n",
    "from outliers import _rm_nonstat_maps, _rm_extreme_maps, _rm_duplicates_maps\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', module='nimare')\n",
    "# Slide on motivations with repect to Neurvault data. potential use cases for this nmotebook.\n",
    "# Soince peole are going to be using it for their own analyss in the fututerr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e5f345",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = op.abspath(\"../data\")\n",
    "filename = \"november_2022\"\n",
    "nv_data_dir = op.join(data_dir, filename)\n",
    "image_dir = op.join(data_dir, \"nv_images\")\n",
    "report_dir = op.join(data_dir, \"report\")\n",
    "report_clean_dir = op.join(data_dir, \"report_clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66ad6c1",
   "metadata": {},
   "source": [
    "# Download Neurovault data\n",
    "\n",
    "To explore the NeuroVault database, we created an SQL query and exported the database contents to human-readable tables while filtering sensitive user information. This provided sufficient metadata from all collections and images to investigate the entire database without downloading the files. The images identified as usable for IBMA (see the following section on the image selection framework) were downloaded along with their metadata and converted to a NiMARE Dataset object to leverage existing IBMA methods implemented in NiMARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48caca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "github_url = (\n",
    "    f\"https://raw.githubusercontent.com/NeuroVault/nv-data/master/{filename}.tar.gz\"\n",
    ")\n",
    "response = requests.get(github_url)\n",
    "\n",
    "file_path = op.join(data_dir, f\"{filename}.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135306ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, \"wb\") as f:\n",
    "    f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a301fa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tarfile.open(file_path, \"r:gz\") as tar:\n",
    "    tar.extractall(path=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21570bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(os.listdir(nv_data_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5f0131",
   "metadata": {},
   "source": [
    "The files availabels are the following:\n",
    "\n",
    "- `django_content_type.csv`: \n",
    "- `statmaps_atlas.csv`: Contain some atlases available in NeuroVaults\n",
    "- `statmaps_basecollectionitem.csv`: Contain the image IDs, name, description and its associated collections ID\n",
    "- `statmaps_cognitiveatlascontrast.csv`: Contains the contrast names and Ids from Cognitive Atlas\n",
    "- `statmaps_cognitiveatlastask.csv`: Contains the task names and Ids from Cognitive Atlas\n",
    "- `statmaps_collection.csv`: Contain the collection level metadata, including name, DOI, authors, etc.\n",
    "- `statmaps_collection_communities.csv`: Contains the comunities link to a collection\n",
    "- `statmaps_collection_contributors.csv`: Contains the user ID link to a colleciton\n",
    "- `statmaps_community.csv`: Contain the information about communities in NeuroVault, such as their label and description\n",
    "- `statmaps_image.csv`: Contain image file name and demographic information linked to the image\n",
    "- `statmaps_statisticmap.csv`: COntain image metadata such as map type, modality, task, analysis leve, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4efe198",
   "metadata": {},
   "source": [
    "To identify images usable for IBMA were first get the images ID (`id`) and name (`name`), and their affiliated collection ID (`collection_id`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8b242e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get images and collection IDs\n",
    "cmeta_cols = [\"id\", \"collection_id\", \"name\"]\n",
    "image_colection = pd.read_csv(\n",
    "    op.join(nv_data_dir, \"statmaps_basecollectionitem.csv\"),\n",
    "    usecols=cmeta_cols,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c19ab8",
   "metadata": {},
   "source": [
    "Second, from `statmaps_image.csv` we get the file name, which will use later to download the images using the NuroVault API. In this database `basecollectionitem_ptr_id` refers to the image ID `id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc35780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get path to images in NeuroVault\n",
    "image_cols = [\"file\", \"basecollectionitem_ptr_id\"]\n",
    "image = pd.read_csv(\n",
    "    op.join(nv_data_dir, \"statmaps_image.csv\"),\n",
    "    usecols=image_cols,\n",
    ")\n",
    "image = image.rename(columns={\"basecollectionitem_ptr_id\": \"id\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc6913c",
   "metadata": {},
   "source": [
    "Third, extract some relevant images metadata from `statmaps_statisticmap.csv`. \n",
    "\n",
    "The selected metadata included:\n",
    "- `image_ptr_id`: Image ID in NeuroVault\n",
    "\n",
    "- `map_type`: Type of statistic that is the basis of the inference\n",
    "    - Possible cases: `T map`, `Z map`, `F map`, `Chi squared map`, `P map (given null hypothesis)`, `1-P map (\"inverted\" probability)`, `multivariate-beta map`, `Beta map`, `ROI/mask`, `parcellation`, `anatomical`, `variance`, `Other`\n",
    "\n",
    "- `modality`: Brain imaging procedure that was used to acquire the data\n",
    "    - Possible cases: `fMRI-BOLD`, `fMRI-CBF`, `fMRI-CBV`, `Diffusion MRI`, `Structural MRI`, `PET FDG`, `PET [150]-water`, `MEG`, `EEG`, `Other`\n",
    "\n",
    "- `analysis_level`: What level of summary data was used as the input to this analysis?\n",
    "    - Possible cases: `single-subject`, `group`, `meta-analysis`, `other`\n",
    "\n",
    "- `number_of_subjects`: Number of subjects used to generate this map\n",
    "    - Integer values ranging from 1 to N\n",
    "\n",
    "- `is_thresholded`: Whether the map is thresholded or not\n",
    "    - Possible cases: `True`, `False`\n",
    "\n",
    "- `brain_coverage`: Percentage of brain coverage\n",
    "\n",
    "- `not_mni`: Whether the image is in the MNI space or not\n",
    "    - Possible cases: `True`, `False`\n",
    "\n",
    "- `cognitive_paradigm_cogatlas_id`: Task performed by the subjects in the scanner described using [Cognitive Atlas](https://www.cognitiveatlas.org/)\n",
    "\n",
    "    One can find the comlete list of Cognitive Atlas task and thier IDs in the file `statmaps_cognitiveatlastask.csv`, or in the Cognitive Atlas [website](https://www.cognitiveatlas.org/tasks), which are also acccesible via [API](https://www.cognitiveatlas.org/api/v-alpha/task)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57d35fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get image metadata\n",
    "imeta_cols = [\n",
    "    \"image_ptr_id\",\n",
    "    \"map_type\",\n",
    "    \"modality\",\n",
    "    \"analysis_level\",\n",
    "    \"number_of_subjects\",\n",
    "    \"is_thresholded\",\n",
    "    \"brain_coverage\",\n",
    "    \"not_mni\",\n",
    "    \"cognitive_paradigm_cogatlas_id\",\n",
    "]\n",
    "image_meta = pd.read_csv(\n",
    "    op.join(nv_data_dir, \"statmaps_statisticmap.csv\"),\n",
    "    usecols=imeta_cols,\n",
    ")\n",
    "image_meta = image_meta.rename(columns={\"image_ptr_id\": \"id\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a4f773",
   "metadata": {},
   "source": [
    "Finally, we combine the previous extracted information into a single Pnadas DataFrame by merging on the image ID (`id`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c524b66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_merge = pd.merge(image_colection, image, how=\"left\", on=\"id\")\n",
    "image_full_df = pd.merge(image_merge, image_meta, how=\"left\", on=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fb5492",
   "metadata": {},
   "source": [
    "# Select images for IBMA\n",
    "\n",
    "Using the available metadata from the retrieved tables, we set different inclusion criteria for images to be considered for a meta-analysis. We focused on fMRI-BOLD images, as they are the most prevalent modality in NeuroVault. Note that the methods presented in this tutorial should work with other image modalities (e.g., PET, diffusion MRI, structural MRI). Still, only fMRI-BOLD had enough data in NeuroVault for meta-analyses. Then, we specifically chose images from group-level analyses. Additionally, we retained only images from studies with a sample size greater than ten subjects. Next, we selected images classified as T or Z statistics. Although best practices in meta-analysis suggest using meaningful units and incorporating uncertainty through standard errors, T/Z statistic maps are the most commonly shared images in NeuroVault (Maumet and Nichols, 2016). We discuss this further in the following sections. Following that, we retained unthresholded images that cover 40% of the brain and are in MNI space. Ultimately, we narrowed our selection to images associated with a Cognitive Atlas task.\n",
    "\n",
    "For this tutorial we use the n-back task (`tsk_4a57abb949bcd`) as it has been validated before (Peraza et al., 2025). Users are ecourage to investigate other domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b312dcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify columns definitions\n",
    "image_selected_df = image_full_df.query(\n",
    "    'modality == \"fMRI-BOLD\"'\n",
    "    ' & analysis_level == \"G\"'\n",
    "    ' & number_of_subjects > 10'\n",
    "    ' & (map_type == \"Z\" | map_type == \"T\")'\n",
    "    ' & is_thresholded == \"f\"'\n",
    "    ' & brain_coverage > 40'\n",
    "    ' & not_mni == \"f\"'\n",
    "    ' & cognitive_paradigm_cogatlas_id == \"tsk_4a57abb949bcd\"'  # n-back task\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3727f4a8",
   "metadata": {},
   "source": [
    "# Download images for IBMA\n",
    "\n",
    "Next, we attempt to downlod the selected image from NeuroVault, to facilitate that process we implemented a function named download images, which take the images IDs from the previously created Python DataFrame, and a path to a directory to download the images. Note that not all images can be downloaded, some ogf them belong to private collection, while others have a corrupted file.\n",
    "\n",
    "Given that some images will have the exact same image name (e.g., z_stats.nii.gz), we downloaded the images adding a uniques identifiable prefix compose by the collection ID and the image ID. THis is, `[col_id]-[img_id]_[image_name].nii.gz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0d9d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = image_selected_df[\"id\"].unique()\n",
    "image_usable_df = download_images(image_ids, image_dir)\n",
    "image_df = pd.merge(image_selected_df, image_usable_df, on=\"id\")\n",
    "\n",
    "print(f\"Usable images: {len(image_usable_df)}/{len(image_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9697c536",
   "metadata": {},
   "source": [
    "# Create NiMARE Dataset object for IBMA\n",
    "\n",
    "THen, we create\n",
    "For users convinience, we have implementd a functiona that facilitate that precess. THe funciton take a Pandas DataFrame and return a NiMARE Databse object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b586255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making explicit we are trasforming Z->T and T->Z\n",
    "# Explain why we are using T/Z\n",
    "dset = convert_to_nimare_dataset(image_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0c1774",
   "metadata": {},
   "source": [
    "# Initialize an IBMA workflow with NiMARE\n",
    "\n",
    "The parameter to initialize the workflow are:\n",
    "\n",
    "- `estimator`: Meta-analysis estimator. Default Stouffers\n",
    "\n",
    "- `corrector`: Meta-analysis corrector. Default FDR\n",
    "\n",
    "- `diagnostics`: Diagnostic method. Default Jackknife\n",
    "\n",
    "- `voxel_thresh`: An optional voxel-level threshold that may be applied to the ``target_image`` of the diagnostic class to define clusters. Default 1.6\n",
    "\n",
    "- `cluster_threshold`: Cluster size threshold. Default 10\n",
    "\n",
    "- `output_dir`: Output directory in which to save results. Default None\n",
    "\n",
    "- `n_cores`: Number of cores to use for parallelization. Default 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102900e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using all the default for teaching popurse\n",
    "# List the default so that people have an understandin of the flexibility\n",
    "workflow = IBMAWorkflow(\n",
    "    estimator=Stouffers(),\n",
    "    corrector=\"fdr\",\n",
    "    diagnostics=\"jackknife\",\n",
    "    voxel_thresh=3.2,\n",
    "    cluster_threshold=90,\n",
    "    output_dir=None,\n",
    "    n_cores=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53af63d6",
   "metadata": {},
   "source": [
    "# Apply workflow to a the dataset object\n",
    "\n",
    "The fit method of a IBMA workflow class runs the following steps:\n",
    "\n",
    "1. Runs a meta-analysis using the specified method (default: Stouffers)\n",
    "\n",
    "2. Applies a corrector to the meta-analysis results (default: FDRCorrector, indep)\n",
    "\n",
    "3. Generates cluster tables and runs diagnostics on the corrected results (default: Jackknife)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8edea1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = workflow.fit(dset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da05a274",
   "metadata": {},
   "source": [
    "# Generate HTML report of IBMA results\n",
    "\n",
    "Finally, a NiMARE report is generated from the MetaResult. The `run_reports` function takes two arguments: (1) A [MetaResult](https://nimare.readthedocs.io/en/stable/generated/nimare.results.MetaResult.html#nimare.results.MetaResult) object produced by the IBMA workflow, and (2) a path to an output directory in which to save the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5bb5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_reports(results, report_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461b3eb5",
   "metadata": {},
   "source": [
    "# Eclude spurious images \n",
    "\n",
    "## Identify and exclude non-statistical maps\n",
    "\n",
    "Even after applying the previous strict preliminary inclusion criteria, we still found plenty of wrongly annotated images, especially representing other image modalities and others with extreme values. Therefore, we developed an automatic heuristic selection to remove those spurious images from the meta-analysis. The heuristic selection consisted of two steps. First, we removed all images from collections that lacked a link to a publication. Also, images with a minimum Z value smaller than 1.96 (i.e., Z score for a 0.05 p-value) were removed as they potentially consisted of mislabeled correlation maps, inverted p-value maps, or did not contain voxels statistically significant. We also excluded images with a maximum Z score larger than 50. Although the number 50 is arbitrary, we wanted to detect images with an unusually large signal. For example, mislabeled BOLD or COPE (contrast of parameter estimates) images or others resulting from studies with a huge sample size. Additionally, using the image metadata, we analyzed the image and file name. We removed those containing keywords such as \"ICA,\" \"PCA,\" \"PPI,\" \"seed,\" \"functional connectivity,\" “cope,” “tfce,” and \"correlation,\" which represent modalities not of interest for the meta-analysis of the current work. Example: https://neurovault.org/images/31938/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be66dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_clean_df = _rm_nonstat_maps(image_df, verbose=1)\n",
    "image_clean_df = _rm_extreme_maps(image_clean_df, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e302555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_clean = convert_to_nimare_dataset(image_clean_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca240412",
   "metadata": {},
   "source": [
    "## Identify and exclude duplicate and inverted contrast images\n",
    "\n",
    "It is quite common for NeuroVault users to upload inverted contrasts and duplicates. For example, one might find two images representing the same contrast (such as House > Face) but with the signs reversed (i.e., Face > House). Example of inverted contrasts in NeuroVault: [MAINFOOD_HCplus>HCmin](https://neurovault.org/images/123498/) and [MAINFOOD_HCmin>HCplus](https://neurovault.org/images/123499/). This creates problems for meta-analyses, as these images effectively cancel each other out when aggregated. \n",
    "\n",
    "Additionally, it is typical for users to upload multiple images of the same contrast, differing only by the covariate used in the group-level analysis. These can be considered duplicates, especially when the covariate does not influence the final estimate. To identify duplicates, we utilize the correlation matrix of the input samples. Image pairs with a correlation close to 1 are considered duplicates, while those with a correlation close to -1 are labeled as inverted contrasts. From the identified duplicates, we randomly selected one image from each pair. For pairs of inverted contrasts, we choose the image with the bigger correlation relative to the median image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fd3ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_clean = _rm_duplicates_maps(dset_clean, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a805264",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_clean = IBMAWorkflow(\n",
    "    estimator=Stouffers(),\n",
    "    corrector=\"fdr\",\n",
    "    diagnostics=\"jackknife\",\n",
    "    voxel_thresh=3.2,\n",
    "    cluster_threshold=90,\n",
    "    output_dir=None,\n",
    "    n_cores=1,\n",
    ")\n",
    "results_clean = workflow_clean.fit(dset_clean)\n",
    "run_reports(results_clean, report_clean_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7211d62b",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "Now, is you turn to work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87fdf66",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "For additional detail on the methods presented in this tutorial plase refer to our preprint: https://doi.org/10.1101/2025.03.06.641922\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
