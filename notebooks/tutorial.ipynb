{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bed8aed8",
   "metadata": {},
   "source": [
    "# OHBM 2025 IBMA with NeuroVault tutorial\n",
    "\n",
    "## IBMA: Image-based meta-analysis\n",
    "\n",
    "IBMA, considered the gold standard of neuroimaging meta-analysis (Salimi-Khorshidi et al., 2009; Salo et al., 2023), consists of aggregating results from group-level, whole-brain statistical maps from individually conducted functional magnetic resonance imaging (fMRI) studies. IBMA outperforms other popular meta-analysis methods, such as coordinate-based meta-analysis (CBMA). IBMA methods use whole-brain statistics; thus, all existing voxel-wise statistical methods are available to analyze subject-level data within studies (Lazar et al., 2002). IBMA is known to produce richer and more detailed results, with additional brain structures that are often absent from CBMA results. IBMA also has greater power; thus, one could potentially achieve similar or even better results with a small fraction of studies generally required in CBMA. In addition, when both the parameters and variance estimates are available, hierarchical mixed effect models can be used to account for both within- and between-study variance (Salimi-Khorshidi et al., 2009).\n",
    "\n",
    "## Tools\n",
    "\n",
    "### NiMARE\n",
    "\n",
    "![NiMARE banner](images/nimare_banner.png)\n",
    "\n",
    "[NiMARE](https://nimare.readthedocs.io/en/latest/) is a Python library for performing neuroimaging meta-analyses and related analyses, like automated annotation and functional decoding. The goal of NiMARE is to centralize and standardize implementations of common meta-analytic tools, so that researchers can use whatever tool is most appropriate for a given research question.\n",
    "\n",
    "### NeuroVault\n",
    "\n",
    "![NeuroVault logo](images/neurovault-logo.svg)\n",
    "\n",
    "[NeuroVault](https://neurovault.org) is a web-based repository of fMRI statistical maps from neuroimaging studies (Gorgolewski et al., 2015). The brain maps are grouped in collections that are created and updated voluntarily. This repository can be explored and downloaded with the help of an API, which is supported by some Python neuroimaging tools (e.g., Nilearn and NiMARE).\n",
    "\n",
    "### Cognitive Atlas\n",
    "\n",
    "![CogAt logo](images/cogat-logo.png)\n",
    "\n",
    "[Cognitive Atlas](https://www.cognitiveatlas.org/) (Poldrack et al., 2011) is an online repository of cumulative knowledge from experienced researchers from the psychology, cognitive science, and neuroscience fields. The repository currently offers two knowledge bases: 907 cognitive concepts and 841 tasks with definitions and properties. Cognitive concepts contain relationships with other concepts and tasks, with the goal of establishing a map between mental processes and brain function. It provides an API to download the database, which is also integrated into NiMARE.\n",
    "\n",
    "## Goals for this tutorial\n",
    "\n",
    "1. Download data from NeuroVault\n",
    "2. Identify usable Neurovault images for IBMA\n",
    "3. Learn to identify outliers\n",
    "4. Use NiMARE to run IBMA\n",
    "5. Interpret IBMA results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46d986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os.path as op\n",
    "import warnings\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from nimare.reports.base import run_reports\n",
    "from nimare.workflows import IBMAWorkflow\n",
    "import requests\n",
    "from nimare.meta.ibma import Stouffers\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn.plotting import plot_stat_map\n",
    "\n",
    "from utils import download_images, convert_to_nimare_dataset\n",
    "from outliers import _rm_nonstat_maps, _rm_extreme_maps, _rm_duplicates_maps\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger(\"nimare\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e5f345",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = op.abspath(\"../data\")\n",
    "filename = \"november_2022\"\n",
    "nv_data_dir = op.join(data_dir, filename)\n",
    "image_dir = op.join(data_dir, \"nv_images\")\n",
    "report_dir = op.join(data_dir, \"report\")\n",
    "report_clean_dir = op.join(data_dir, \"report_clean\")\n",
    "\n",
    "os.makedirs(data_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66ad6c1",
   "metadata": {},
   "source": [
    "Download Neurovault data\n",
    "\n",
    "To explore the NeuroVault database, we created an SQL query and exported the database contents to human-readable tables while filtering sensitive user information. This provided sufficient metadata from all collections and images to investigate the entire database without downloading the files. The images identified as usable for IBMA (see the following section on the image selection framework) were downloaded along with their metadata and converted to a NiMARE Dataset object to leverage existing IBMA methods implemented in NiMARE.\n",
    "\n",
    "**Note**: The following three cells, which download the NeuroVault data tables, need to be run only once in this tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48caca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "github_url = (\n",
    "    f\"https://raw.githubusercontent.com/NeuroVault/nv-data/master/{filename}.tar.gz\"\n",
    ")\n",
    "response = requests.get(github_url)\n",
    "\n",
    "file_path = op.join(data_dir, f\"{filename}.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135306ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, \"wb\") as f:\n",
    "    f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a301fa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tarfile.open(file_path, \"r:gz\") as tar:\n",
    "    tar.extractall(path=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21570bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(os.listdir(nv_data_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5f0131",
   "metadata": {},
   "source": [
    "The files available are the following:\n",
    "\n",
    "- `django_content_type.csv`: No relevant\n",
    "- `statmaps_atlas.csv`: Contains some atlases available in NeuroVaults\n",
    "- `statmaps_basecollectionitem.csv`: Contains the image IDs, name, description, and the associated collection ID\n",
    "- `statmaps_cognitiveatlascontrast.csv`: Contains the contrast names and IDs from Cognitive Atlas\n",
    "- `statmaps_cognitiveatlastask.csv`: Contains the task names and IDs from Cognitive Atlas\n",
    "- `statmaps_collection.csv`: Contains the collection-level metadata, including name, DOI, authors, etc.\n",
    "- `statmaps_collection_communities.csv`: Contains the communities linked to a collection\n",
    "- `statmaps_collection_contributors.csv`: Contains the user ID linked to a collection\n",
    "- `statmaps_community.csv`: Contains the information about communities in NeuroVault, such as their label and description\n",
    "- `statmaps_image.csv`: Contains image file name and demographic information linked to the image\n",
    "- `statmaps_statisticmap.csv`: Contains image metadata such as map type, modality, task, analysis level, etc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4efe198",
   "metadata": {},
   "source": [
    "To identify images usable for IBMA, first get the image's ID (`id`) and name (`name`), as well as their affiliated collection ID (`collection_id`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8b242e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get images and collection IDs\n",
    "cmeta_cols = [\"id\", \"collection_id\", \"name\"]\n",
    "image_colection = pd.read_csv(\n",
    "    op.join(nv_data_dir, \"statmaps_basecollectionitem.csv\"),\n",
    "    usecols=cmeta_cols,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c19ab8",
   "metadata": {},
   "source": [
    "Second, from `statmaps_image.csv` we get the file name, which will use later to download the images using the NuroVault API. In this database `basecollectionitem_ptr_id` refers to the image ID `id`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc35780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get path to images in NeuroVault\n",
    "image_cols = [\"file\", \"basecollectionitem_ptr_id\"]\n",
    "image = pd.read_csv(\n",
    "    op.join(nv_data_dir, \"statmaps_image.csv\"),\n",
    "    usecols=image_cols,\n",
    ")\n",
    "image = image.rename(columns={\"basecollectionitem_ptr_id\": \"id\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc6913c",
   "metadata": {},
   "source": [
    "Third, some relevant images and metadata are extracted from `statmaps_statisticmap.csv`.\n",
    "\n",
    "The selected metadata included:\n",
    "\n",
    "- `image_ptr_id`: Image ID in NeuroVault\n",
    "\n",
    "- `map_type`: Type of statistic that is the basis of the inference\n",
    "\n",
    "  - Possible cases: `T map`, `Z map`, `F map`, `Chi squared map`, `P map (given null hypothesis)`, `1-P map (\"inverted\" probability)`, `multivariate-beta map`, `Beta map`, `ROI/mask`, `parcellation`, `anatomical`, `variance`, `Other`\n",
    "\n",
    "- `modality`: Brain imaging procedure that was used to acquire the data\n",
    "\n",
    "  - Possible cases: `fMRI-BOLD`, `fMRI-CBF`, `fMRI-CBV`, `Diffusion MRI`, `Structural MRI`, `PET FDG`, `PET [150]-water`, `MEG`, `EEG`, `Other`\n",
    "\n",
    "- `analysis_level`: What level of summary data was used as the input to this analysis?\n",
    "\n",
    "  - Possible cases: `single-subject`, `group`, `meta-analysis`, `other`\n",
    "\n",
    "- `number_of_subjects`: Number of subjects used to generate this map\n",
    "\n",
    "  - Integer values ranging from 1 to N\n",
    "\n",
    "- `is_thresholded`: Whether the map is thresholded or not\n",
    "\n",
    "  - Possible cases: `True`, `False`\n",
    "\n",
    "- `brain_coverage`: Percentage of brain coverage\n",
    "\n",
    "- `not_mni`: Whether the image is in the MNI space or not\n",
    "\n",
    "  - Possible cases: `True`, `False`\n",
    "\n",
    "- `cognitive_paradigm_cogatlas_id`: Task performed by the subjects in the scanner described using [Cognitive Atlas](https://www.cognitiveatlas.org/)\n",
    "\n",
    "  **Note**: One can find the complete list of Cognitive Atlas tasks and their IDs in the file `statmaps_cognitiveatlastask.csv`, or in the Cognitive Atlas [website](https://www.cognitiveatlas.org/tasks), which are also accessible via [API](https://www.cognitiveatlas.org/api/v-alpha/task).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57d35fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get image metadata\n",
    "imeta_cols = [\n",
    "    \"image_ptr_id\",\n",
    "    \"map_type\",\n",
    "    \"modality\",\n",
    "    \"analysis_level\",\n",
    "    \"number_of_subjects\",\n",
    "    \"is_thresholded\",\n",
    "    \"brain_coverage\",\n",
    "    \"not_mni\",\n",
    "    \"cognitive_paradigm_cogatlas_id\",\n",
    "]\n",
    "image_meta = pd.read_csv(\n",
    "    op.join(nv_data_dir, \"statmaps_statisticmap.csv\"),\n",
    "    usecols=imeta_cols,\n",
    ")\n",
    "image_meta = image_meta.rename(columns={\"image_ptr_id\": \"id\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a4f773",
   "metadata": {},
   "source": [
    "Finally, we combine the previously extracted information into a single Pandas DataFrame by merging on the image ID (`id`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c524b66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_merge = pd.merge(image_colection, image, how=\"left\", on=\"id\")\n",
    "image_full_df = pd.merge(image_merge, image_meta, how=\"left\", on=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fb5492",
   "metadata": {},
   "source": [
    "# Select images for IBMA\n",
    "\n",
    "Using the available metadata from the retrieved tables, we set different inclusion criteria for images to be considered for a meta-analysis. We focused on fMRI-BOLD images, as they are the most prevalent modality in NeuroVault. Note that the methods presented in this tutorial should work with other image modalities (e.g., PET, diffusion MRI, structural MRI). Still, only fMRI-BOLD had enough data in NeuroVault for meta-analyses. Then, we specifically chose images from group-level analyses. Additionally, we retained only images from studies with a sample size greater than ten subjects. Next, we selected images classified as T or Z statistics. Although best practices in meta-analysis suggest using meaningful units and incorporating uncertainty through standard errors, T/Z statistic maps are the most commonly shared images in NeuroVault (Maumet and Nichols, 2016). We discuss this further in the following sections. Following that, we retained unthresholded images that cover 40% of the brain and are in MNI space. Ultimately, we narrowed our selection to images associated with a Cognitive Atlas task.\n",
    "\n",
    "For this tutorial, we use the n-back task (`tsk_4a57abb949bcd`) as it has been validated before (Peraza et al., 2025). Users are encouraged to investigate other domains.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b312dcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_selected_df = image_full_df.query(\n",
    "    'modality == \"fMRI-BOLD\"'\n",
    "    ' & analysis_level == \"G\"'\n",
    "    \" & number_of_subjects > 10\"\n",
    "    ' & (map_type == \"Z\" | map_type == \"T\")'\n",
    "    ' & is_thresholded == \"f\"'\n",
    "    \" & brain_coverage > 40\"\n",
    "    ' & not_mni == \"f\"'\n",
    "    ' & cognitive_paradigm_cogatlas_id == \"tsk_4a57abb949bcd\"'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3727f4a8",
   "metadata": {},
   "source": [
    "# Download images for IBMA\n",
    "\n",
    "Next, we attempt to download the selected image from NeuroVault. To facilitate that process, we implemented a function named download images, which takes the image IDs from the previously created Python DataFrame and a path to a directory to download the images. Note that not all images can be downloaded; some of them belong to a private collection, while others have a corrupted file.\n",
    "\n",
    "Given that some images will have the same image name (e.g., z*stats.nii.gz), we downloaded the images, adding a unique identifiable prefix composed of the collection ID and the image ID. THis is, `[col_id]-[img_id]*[image_name].nii.gz`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0d9d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = image_selected_df[\"id\"].unique()\n",
    "image_usable_df = download_images(image_ids, image_dir)\n",
    "image_df = pd.merge(image_selected_df, image_usable_df, on=\"id\")\n",
    "\n",
    "print(f\"Usable images: {len(image_usable_df)}/{len(image_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9697c536",
   "metadata": {},
   "source": [
    "# Create NiMARE Dataset object for IBMA\n",
    "\n",
    "Then, we create a NiMARE Dataset object in order to leverage image-based meta-analytic methods from NiMARE. In NiMARE, datasets are stored in a special `Dataset` class. The `Dataset` class stores the most relevant information as properties.\n",
    "\n",
    "For users' convenience, we have implemented a function `convert_to_nimare_dataset` to facilitate that process. The function takes a Pandas DataFrame and returns a NiMARE Database object.\n",
    "\n",
    "In addition, we use the `ImageTransformer`, which can generate images from other images, as long as the right images and metadata are available. In this case, we generate z-statistic images from t-statistic maps, leveraging the sample size information in the metadata.\n",
    "\n",
    "For additional detail on creating a Dataset class, please refer to this example: https://nimare.readthedocs.io/en/stable/auto_examples/01_datasets/06_plot_dataset_json.html#sphx-glr-auto-examples-01-datasets-06-plot-dataset-json-py.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b586255",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = convert_to_nimare_dataset(image_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fd815a",
   "metadata": {},
   "source": [
    "The full list of identifiers in the Dataset is located in `Dataset.ids`. Identifiers are composed of two parts- a study ID and a contrast ID. Within the Dataset, those two parts are separated with a `-`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca396717",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dset.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c920d20b",
   "metadata": {},
   "source": [
    "Most other information is stored in `pandas` DataFrames. The five DataFrame-based attributes are `Dataset.metadata`, `Dataset.coordinates`, `Dataset.images`, `Dataset.annotations`, and `Dataset.texts`.\n",
    "\n",
    "Each DataFrame contains at least three columns: `study_id`, `contrast_id`, and `id`, which is the combined `study_id` and `contrast_id`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff36e34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset.metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982df668",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset.images.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0c1774",
   "metadata": {},
   "source": [
    "# Initialize an IBMA workflow with NiMARE\n",
    "\n",
    "Next, we leverage the available NiMARE workflows that implements several meta-analytic pipelines. Specifically, we use an IBMAWorkflow\n",
    "\n",
    "The parameters to initialize the workflow are:\n",
    "\n",
    "- `estimator`: Meta-analysis estimator. Default Stouffers\n",
    "\n",
    "  Useful parameter in Stouffers:\n",
    "\n",
    "  - `aggressive_mask`: By default, all image-based meta-analysis estimators adopt an aggressive masking strategy, in which any voxels with a value of zero in any of the input maps will be removed from the analysis. Setting `aggressive_mask=False` will instead run the analysis in bags of voxels that have a valid value across the same studies.\n",
    "\n",
    "  - `use_sample_size`: Whether to use sample sizes for weights (i.e., \"weighted Stouffer's\") or not.\n",
    "\n",
    "  - `normalize_contrast_weights`: Whether to use a number of contrasts per study to normalize the weights or not.\n",
    "\n",
    "  **Note**: The current implementation in NiMARE adds a correction factor to account for repeated scans (i.e., multiple samples from the same study).\n",
    "\n",
    "- `corrector`: Meta-analysis corrector. Default FDR\n",
    "\n",
    "- `diagnostics`: Diagnostic method. Default Jackknife\n",
    "\n",
    "- `voxel_thresh`: An optional voxel-level threshold that may be applied to the `target_image` of the diagnostic class to define clusters. Default 1.6\n",
    "\n",
    "- `cluster_threshold`: Cluster size threshold. Default 10\n",
    "\n",
    "- `output_dir`: Output directory in which to save results. Default None\n",
    "\n",
    "- `n_cores`: Number of cores to use for parallelization. Default 1\n",
    "\n",
    "  Here, parallelization only applies to the Jackknife analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102900e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = IBMAWorkflow(\n",
    "    estimator=Stouffers(\n",
    "        aggressive_mask=True,\n",
    "        use_sample_size=True,\n",
    "        normalize_contrast_weights=True,\n",
    "    ),\n",
    "    corrector=\"fdr\",\n",
    "    diagnostics=\"jackknife\",\n",
    "    voxel_thresh=3.2,\n",
    "    cluster_threshold=90,\n",
    "    output_dir=None,\n",
    "    n_cores=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53af63d6",
   "metadata": {},
   "source": [
    "# Apply workflow to the Dataset object\n",
    "\n",
    "The fit method of an IBMA workflow class runs the following steps:\n",
    "\n",
    "1. Runs a meta-analysis using the specified method (default: Stouffers)\n",
    "\n",
    "2. Applies a corrector to the meta-analysis results (default: FDRCorrector, indep)\n",
    "\n",
    "3. Generates cluster tables and runs diagnostics on the corrected results (default: Jackknife)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8edea1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = workflow.fit(dset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661639c3",
   "metadata": {},
   "source": [
    "# Plot Results\n",
    "\n",
    "The fit method of the IBMA workflow class returns a MetaResult object, where you can access the corrected results of the meta-analysis and diagnostics tables.\n",
    "\n",
    "Corrected map:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b959b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = results.get_map(\"z_corr-FDR_method-indep\")\n",
    "plot_stat_map(\n",
    "    img,\n",
    "    cut_coords=4,\n",
    "    display_mode=\"z\",\n",
    "    threshold=1.65,  # voxel_thresh p < .05, one-tailed\n",
    "    cmap=\"RdBu_r\",\n",
    "    symmetric_cbar=True,\n",
    "    vmax=4,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50e029f",
   "metadata": {},
   "source": [
    "## Clusters table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bf521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.tables[\"z_corr-FDR_method-indep_tab-clust\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707e2f50",
   "metadata": {},
   "source": [
    "## Diagnostic: Jackknife\n",
    "\n",
    "The Jackknife analysis characterizes the relative contribution of each experiment in a meta-analysis to the resulting clusters by looping through experiments, calculating the Estimator's summary statistic for all experiments except the target experiment, dividing the resulting test summary statistics by the summary statistics from the original meta-analysis, and finally averaging the resulting proportion values across all voxels in each cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d37ca55",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.tables[\"z_corr-FDR_method-indep_diag-Jackknife_tab-counts\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da05a274",
   "metadata": {},
   "source": [
    "# Generate an HTML report of IBMA results\n",
    "\n",
    "Finally, a NiMARE report is generated from the MetaResult. The `run_reports` function takes two arguments: (1) A [MetaResult](https://nimare.readthedocs.io/en/stable/generated/nimare.results.MetaResult.html#nimare.results.MetaResult) object produced by the IBMA workflow, and (2) a path to an output directory in which to save the report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5bb5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_reports(results, report_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461b3eb5",
   "metadata": {},
   "source": [
    "# Exclude spurious images\n",
    "\n",
    "## Identify and exclude non-statistical maps\n",
    "\n",
    "Even after applying the previous strict preliminary inclusion criteria, we still found plenty of wrongly annotated images, especially representing other image modalities and others with extreme values. Therefore, we developed an automatic heuristic selection to remove those spurious images from the meta-analysis. The heuristic selection consisted of two steps. First, we removed all images from collections that lacked a link to a publication. Also, images with a minimum Z value smaller than 1.96 (i.e., Z score for a 0.05 p-value) were removed as they potentially consisted of mislabeled correlation maps, inverted p-value maps, or did not contain statistically significant voxels. We also excluded images with a maximum Z score larger than 50. Although the number 50 is arbitrary, we wanted to detect images with an unusually large signal. For example, mislabeled BOLD or COPE (contrast of parameter estimates) images or others resulting from studies with a huge sample size. Additionally, using the image metadata, we analyzed the image and file name. We removed those containing keywords such as \"ICA,\" \"PCA,\" \"PPI,\" \"seed,\" \"functional connectivity,\" “cope,” “tfce,” and \"correlation,\" which represent modalities not of interest for the meta-analysis of the current work. Example: https://neurovault.org/images/31938/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be66dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_clean_df = _rm_nonstat_maps(image_df, verbose=1)\n",
    "image_clean_df = _rm_extreme_maps(image_clean_df, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e302555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_clean = convert_to_nimare_dataset(image_clean_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca240412",
   "metadata": {},
   "source": [
    "## Identify and exclude duplicate and inverted contrast images\n",
    "\n",
    "It is quite common for NeuroVault users to upload inverted contrasts and duplicates. For example, one might find two images representing the same contrast (such as House > Face) but with the signs reversed (i.e., Face > House). Example of inverted contrasts in NeuroVault: [MAINFOOD_HCplus>HCmin](https://neurovault.org/images/123498/) and [MAINFOOD_HCmin>HCplus](https://neurovault.org/images/123499/). This creates problems for meta-analyses, as these images effectively cancel each other out when aggregated.\n",
    "\n",
    "Additionally, it is typical for users to upload multiple images of the same contrast, differing only by the covariate used in the group-level analysis. These can be considered duplicates, especially when the covariate does not influence the final estimate. To identify duplicates, we utilize the correlation matrix of the input samples. Image pairs with a correlation close to 1 are considered duplicates, while those with a correlation close to -1 are labeled as inverted contrasts. From the identified duplicates, we randomly selected one image from each pair. For pairs of inverted contrasts, we choose the image with the bigger correlation relative to the median image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fd3ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_clean = _rm_duplicates_maps(dset_clean, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9562b3",
   "metadata": {},
   "source": [
    "Run the IBMA workflow on the clean dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a805264",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_clean = IBMAWorkflow(\n",
    "    estimator=Stouffers(\n",
    "        aggressive_mask=True,\n",
    "        use_sample_size=True,\n",
    "        normalize_contrast_weights=True,\n",
    "    ),\n",
    "    corrector=\"fdr\",\n",
    "    diagnostics=\"jackknife\",\n",
    "    voxel_thresh=3.2,\n",
    "    cluster_threshold=90,\n",
    "    output_dir=None,\n",
    "    n_cores=1,\n",
    ")\n",
    "results_clean = workflow_clean.fit(dset_clean)\n",
    "run_reports(results_clean, report_clean_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7211d62b",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "Now, is you turn to work\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87fdf66",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "For additional detail on the methods presented in this tutorial plase refer to our preprint: https://doi.org/10.1101/2025.03.06.641922\n",
    "\n",
    "- Gorgolewski, K.J., Varoquaux, G., Rivera, G., Schwarz, Y., Ghosh, S.S., Maumet, C., Sochat, V.V., Nichols, T.E., Poldrack, R.A., Poline, J.-B., Yarkoni, T., Margulies, D.S., 2015. NeuroVault.org: a web-based repository for collecting and sharing unthresholded statistical maps of the human brain. Frontiers in Neuroinformatics 9, 8. https://doi.org/10.3389/fninf.2015.00008\n",
    "\n",
    "- Lazar, N.A., Luna, B., Sweeney, J.A., Eddy, W.F., 2002. Combining Brains: A Survey of Methods for Statistical Pooling of Information. NeuroImage 16, 538–550. https://doi.org/10.1006/nimg.2002.1107\n",
    "\n",
    "- Maumet, C., Nichols, T.E., 2016. Minimal Data Needed for Valid & Accurate Image-Based fMRI Meta-Analysis. https://doi.org/10.1101/048249\n",
    "\n",
    "- Peraza, J.A., Kent, J.D., Blair, R.W., Poline, J.-B., Nichols, T.E., Vega, A. de la, Laird, A.R., 2025. Advancing image-based meta-analysis for fMRI: A framework for leveraging NeuroVault data. https://doi.org/10.1101/2025.03.06.641922\n",
    "\n",
    "- Poldrack, R.A., Kittur, A., Kalar, D., Miller, E., Seppa, C., Gil, Y., Parker, D.S., Sabb, F.W., Bilder, R.M., 2011. The cognitive atlas: toward a knowledge foundation for cognitive neuroscience. Front Neuroinform 5, 17. https://doi.org/10.3389/fninf.2011.00017\n",
    "\n",
    "- Salimi-Khorshidi, G., Smith, S.M., Keltner, J.R., Wager, T.D., Nichols, T.E., 2009. Meta-analysis of neuroimaging data: A comparison of image-based and coordinate-based pooling of studies. NeuroImage 45, 810–823. https://doi.org/10.1016/j.neuroimage.2008.12.039\n",
    "\n",
    "- Salo, T., Yarkoni, T., Nichols, T.E., Poline, J.-B., Bilgel, M., Bottenhorn, K.L., Jarecka, D., Kent, J.D., Kimbler, A., Nielson, D.M., Oudyk, K.M., Peraza, J.A., Pérez, A., Reeders, P.C., Yanes, J.A., Laird, A.R., 2023. NiMARE: Neuroimaging Meta-Analysis Research Environment. Aperture Neuro 3, 1–32. https://doi.org/10.52294/001c.87681\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4618fd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
